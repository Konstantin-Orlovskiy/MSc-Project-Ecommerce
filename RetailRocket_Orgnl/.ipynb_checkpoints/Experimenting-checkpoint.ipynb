{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "from scipy.sparse import coo_matrix  # LightFM fit method requires coo matrix format as input.\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Data Import and Cleaning - Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Events data and sorting by timestamp column which corresponds the historical order of events.\n",
    "\n",
    "df_events = pd.read_csv(\"events.csv\")\n",
    "df_events = df_events.sort_values(by=['timestamp'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View on the ratio between different types of events.\n",
    "sns.countplot(x='event', data=df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events types “view”, “addtocart”, “transaction” are the implicit customer feedback.\n",
    "# They can be considered as rating and will be transformed from categorical to numerical format.\n",
    "\n",
    "# The weights are subject to tuning together with hyperparameters to achieve better performance.\n",
    "\n",
    "weight_view = 1\n",
    "weight_addtocart = 2\n",
    "weight_transaction = 3\n",
    "\n",
    "df_events.event.replace(to_replace=dict(\n",
    "    view=weight_view, addtocart=weight_addtocart, transaction=weight_transaction), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now the events replaced with corresponding weights.\n",
    "df_events.event.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The user may have interacted with item multiple times which is now stored in dataframe.\n",
    "# For the purpose of recommendation we're interested in the highest level of user interest to the item.\n",
    "# Therefore, the data can be further cleaned.\n",
    "\n",
    "df_events = df_events.sort_values('event').drop_duplicates(\n",
    "    subset=['visitorid', 'itemid'], \n",
    "    keep='last').sort_values(by=['timestamp'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which types of interaction to be used: weight_view / weight_addtocart / weight_transaction.\n",
    "\n",
    "df_events = df_events.loc[df_events['event'].isin(\n",
    "    [weight_addtocart, \n",
    "     weight_transaction])].reset_index(drop=True)\n",
    "\n",
    "df_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those users that had low activity must be removed from the data.\n",
    "# Considering number of users and items it won't be possible to evaluate predictions having no user features.\n",
    "# Therefore, it makes sense to delete these users, which will remove noise and save computational cost.\n",
    "# At the same time, items with low interactions can be kept as there's item features data available.\n",
    "\n",
    "# !!!\n",
    "# Before production the model should be additionally trained on the interactions of removed users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count activities by user.\n",
    "users_activity = df_events.groupby('visitorid').visitorid.count().to_frame(name='activity_count')\n",
    "users_activity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_activity.loc[users_activity['activity_count'] > 10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.loc[df_events['visitorid'] == 627]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the ratio of users having only low level of interaction.\n",
    "\n",
    "interaction_threshold = 3\n",
    "\n",
    "users_activity_low = users_activity.loc[users_activity['activity_count'] <= interaction_threshold]\n",
    "ratio = len(users_activity_low) / len(users_activity)\n",
    "print('Users with <=', interaction_threshold, 'interactions represent', round(ratio*100,2),\n",
    "      '% of total users and are to be removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of users that need to be removed from events data.\n",
    "users_to_remove = users_activity_low.index.tolist()\n",
    "len(users_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users with only 1 interaction from df_events dataframe.\n",
    "df_events = df_events[~df_events.visitorid.isin(users_to_remove)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final view on users and items participating in model training and testing. \n",
    "\n",
    "qty_all_items = len(df_events['itemid'].unique())\n",
    "print('Cleaned dataset number of items: ', qty_all_items)\n",
    "print()\n",
    "qty_all_users = len(df_events['visitorid'].unique())\n",
    "print('Cleaned dataset number of users: ', qty_all_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use timestamps for split which mimics the real-life case as the events are sorted in historical order.\n",
    "# Split ratio is 80% for train set, and 20% for test set.\n",
    "\n",
    "split_point = int(np.ceil(len(df_events)*0.8))  # Index of split point.\n",
    "split_point_time = int(df_events.loc[split_point]['timestamp'])  # Timestamp of split point.\n",
    "\n",
    "df_events_train = df_events.loc[0:split_point]\n",
    "df_events_test = df_events.loc[split_point+1:]\n",
    "\n",
    "\n",
    "# !!!!!!\n",
    "# Exclude from test set those users and items that are no included in train set.\n",
    "# This avoids facing cold start problem on evaluation phase.\n",
    "df_events_test = df_events_test[(df_events_test['visitorid'].isin(df_events_train['visitorid'])) & \n",
    "                                (df_events_test['itemid'].isin(df_events_train['itemid']))]\n",
    "\n",
    "\n",
    "df_events_train.info()\n",
    "print()\n",
    "df_events_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming interactions data into the format acceptable by lightFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class of LightFM package has method build_interactions that allows to fill in the interactions matrix.\n",
    "# As the input for this method need to pass the list of tuples (visitorid, itemid, weight).\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train set interactions transformed.\n",
    "df_events_train_interactions = []\n",
    "for index, row in df_events_train.iterrows():\n",
    "    df_events_train_interactions.append((int(row['visitorid']), int(row['itemid']), int(row['event'])))\n",
    "    \n",
    "# Test set interactions transformed.\n",
    "df_events_test_interactions = []\n",
    "for index, row in df_events_test.iterrows():\n",
    "    df_events_test_interactions.append((int(row['visitorid']), int(row['itemid']), int(row['event'])))\n",
    "    \n",
    "print('Finished in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original VS transformed length, should be equal.\n",
    "\n",
    "print('Check original VS transformed length TRAIN: ', \n",
    "     len(df_events_train),\n",
    "     '/',\n",
    "     len(df_events_train_interactions))\n",
    "\n",
    "print('Check original VS transformed length TEST: ', \n",
    "     len(df_events_test),\n",
    "     '/',\n",
    "     len(df_events_test_interactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Data Import and Cleaning - Item Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import Properties\n",
    "\n",
    "df_properties1 = pd.DataFrame(pd.read_csv(\"item_properties_part1.csv\"))\n",
    "df_properties2 = pd.DataFrame(pd.read_csv(\"item_properties_part2.csv\"))\n",
    "df_properties = pd.concat([df_properties1, df_properties2])\n",
    "\n",
    "# data to be sorted by timestamp to reflect the historical change log.\n",
    "df_properties = df_properties.sort_values(by=['timestamp'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "df_properties.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_properties.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties_len_orig = len(df_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All the categories names and values are hashed excepting \"categoryid\" and \"available\".\n",
    "\n",
    "df_properties.loc[df_properties['itemid'] == 216269].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is unable to process the historical log so there's a need to trim the properties data.\n",
    "# The latest properties data is considered to be the best to describe items assuming the ecommerce team was \n",
    "# constantly improving the catalogue.\n",
    "# This action should lead to decrease of the dataframe size.\n",
    "\n",
    "df_properties = df_properties.sort_values(by=['timestamp'], ascending=True).drop_duplicates(\n",
    "    subset=['itemid', 'property'], \n",
    "    keep='last').reset_index(drop=True)\n",
    "\n",
    "df_properties.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Additionally, we can get rid of the 'available' property completely.\n",
    "# It won't make sense to consider any value as fixed (in stock or not in stock) for trainig purposes.\n",
    "# In production this property can be used in real time to filter out unavailable items from prediction.\n",
    "\n",
    "df_properties = df_properties[~df_properties.property.isin(['available'])].reset_index(drop=True)\n",
    "\n",
    "df_properties.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the next step the items which are not present in the cleaned dataframe can also be removed.\n",
    "\n",
    "df_properties = df_properties[df_properties.itemid.isin(df_events['itemid'])].reset_index(drop=True)\n",
    "\n",
    "df_properties.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp column can also be removed as it is redundant.\n",
    "\n",
    "df_properties = df_properties.drop(['timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by itemid.\n",
    "\n",
    "df_properties = df_properties.sort_values(by=['itemid'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaned properties dataframe represents ',\n",
    "      round(100*len(df_properties)/df_properties_len_orig,2),\n",
    "     ' % of original dataframe.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qty_all_properties = len(df_properties['property'].unique())\n",
    "print('Cleaned dataset number of properties: ', qty_all_properties)\n",
    "print('Cleaned dataset number of properties values: ', len(df_properties))\n",
    "\n",
    "print('Cleaned dataset number of items: ', qty_all_items)\n",
    "print('Cleaned dataset number of users: ', qty_all_users)\n",
    "print('Cleaned dataset number of interactions: ', len(df_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_properties.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming item features data into the format acceptable by lightFM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item features information should be passed to the lightFM model in a format of csr matrix.\n",
    "# This matrix must have mapping of all items related to the train/test process and \n",
    "# all features available in cleaned properties dataframe.\n",
    "# Then the matrix is to be filled in with the properties values.\n",
    "# LightFM model would be then capable to create a latent features vector for each item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_properties.loc[df_properties['itemid'] == 332253].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset class has the method build_item_features that allows to fill in the properties data using created mapping.\n",
    "# Add feature mapping to the existing dataset using fit_partial method.\n",
    "# Data needs to be transformed to the acceptable format.\n",
    "# https://github.com/lyst/lightfm/issues/393#issuecomment-438237971\n",
    "\n",
    "\n",
    "# Transform item features list to the format required by Dataset class:\n",
    "# ['property:value']\n",
    "item_features_mapping = []\n",
    "for index, row in df_properties.iterrows():\n",
    "    item_features_mapping.append(str(row['property']) + ':' + str(row['value']))\n",
    "    \n",
    "print('Properties mapping has:', len(item_features_mapping), 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the next step need to remove duplicates in this list.\n",
    "# So basically the feature mapping lenght is not equal to the number of features, \n",
    "# but is equal to the number of combinations ['property:value'] available in the dataframe.\n",
    "# There's no need to \"create\" all possible combinations mapping as this is redundant info.\n",
    "# Just need to create mapping for existing combinations.\n",
    "# Additionally, the LightFM package allows to add weight to this combination if needed.\n",
    "\n",
    "item_features_mapping = list( dict.fromkeys(item_features_mapping) )\n",
    "print('Properties mapping has:', len(item_features_mapping), 'records')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Transform item features values to the format required by Dataset class:\n",
    "# [ (itemid_1, ['property_1:value_1']), (itemid_1, 'property_2:value_2']) ]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "item_features_values = []\n",
    "for index, row in df_properties.iterrows():\n",
    "    item_features_values.append(\n",
    "        (row['itemid'], [\n",
    "            str(row['property']) + ':' + str(row['value'])\n",
    "        ]),)\n",
    "\n",
    "\n",
    "print('Finished in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform item features values to the format required by Dataset class:\n",
    "# [ (itemid_1, ['property_1:value_1', 'property_2:value_2']) ]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "item_features_values = []\n",
    "current_item = df_properties.itemid[0]\n",
    "current_item_features = []\n",
    "for index, row in df_properties.iterrows():\n",
    "    if row['itemid'] == current_item:\n",
    "        current_item_features.append(str(row['property']) + ':' + str(row['value']))\n",
    "    else:\n",
    "        item_features_values.append((current_item, current_item_features))\n",
    "        current_item = row['itemid']\n",
    "        current_item_features = [str(row['property']) + ':' + str(row['value'])]\n",
    "item_features_values.append((current_item, current_item_features))\n",
    "\n",
    "print('Finished in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features_values[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Create LightFM dataset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model evaluation purposes (auc_score) dimensionality of train/test interaction matrices should be the same.\n",
    "# In order to achieve this, need to create mapping for all users and all items.\n",
    "# Then separately for train and test - the interactions will be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for users, items and item features.\n",
    "\n",
    "# The fit method of class Dataset takes the list of all the visitors and items.\n",
    "# The implementation allows to ignore duplicates.\n",
    "\n",
    "# Train set mapping.\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users = (x for x in df_events['visitorid']),\n",
    "    items = (x for x in df_events['itemid']), \n",
    "    user_features=None, \n",
    "    item_features=item_features_mapping\n",
    ")\n",
    "\n",
    "dataset_train = dataset\n",
    "dataset_test = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate LightFM dataset with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Populate interactions matrix for train and test sets.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "(interactions_train, weights_train) = dataset_train.build_interactions(df_events_train_interactions)\n",
    "(interactions_test, weights_test) = dataset_test.build_interactions(df_events_test_interactions)\n",
    "\n",
    "print('Finished in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Populate item features matrix with values.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "item_features = dataset.build_item_features(item_features_values)\n",
    "\n",
    "print('Finished in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset class cross-check.')\n",
    "print()\n",
    "\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('All users expected:', qty_all_users)\n",
    "print('Actual number of users:', num_users)\n",
    "print()\n",
    "print('All items expected:', qty_all_items)\n",
    "print('Actual number of items:', num_items)\n",
    "print()\n",
    "\n",
    "print('Item features matrix is expected to be of size: (', qty_all_items, ',', \n",
    "      qty_all_items+len(item_features_mapping), ')')\n",
    "# Reason for this size is that there's a space for latent vector representation of each item plus all features.\n",
    "print('Actual size is:', dataset.item_features_shape())\n",
    "print()\n",
    "\n",
    "print('Item features matrix number of values is expected to be:', len(df_properties)+qty_all_items)\n",
    "# Reason for this size is that there's a space for latent vector representation of each item plus all features.\n",
    "print('Actual number of values is:', item_features.getnnz())\n",
    "print()\n",
    "\n",
    "print('Interactions matrix is expected to be of the size: (', qty_all_users, ',', qty_all_items, ')')\n",
    "print('Actual size is:', dataset.interactions_shape())\n",
    "print()\n",
    "\n",
    "print('Interactions matrix number of values is expected to be:', len(df_events_train) + len(df_events_test))\n",
    "print('Actual number of values is:', weights_train.getnnz() + weights_test.getnnz())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM model training (with item features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with item features. This will mean hybrid predictions.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_hybrid = LightFM(no_components=100, loss='warp', random_state=2020)\n",
    "\n",
    "model_hybrid.fit(weights_train, \n",
    "                 item_features=item_features, \n",
    "                 epochs=100, \n",
    "                 num_threads=4)\n",
    "\n",
    "print('Model trained in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hybrid.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation (auc_score, precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the recommendation engine is a ranking problem, AUC ROC and Precision at K will be used.\n",
    "# Both of them are measuring the ranking quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_auc = auc_score(model_hybrid, \n",
    "                      weights_train, \n",
    "                      item_features=item_features, \n",
    "                      num_threads=4).mean()\n",
    "\n",
    "print('Train AUC score: ', train_auc)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train interactions fill be also passed to avoid model re-recommending items to users.\n",
    "test_auc = auc_score(model_hybrid,\n",
    "                     weights_test, \n",
    "                     item_features=item_features, \n",
    "                     train_interactions = weights_train, \n",
    "                     num_threads=4).mean()\n",
    "\n",
    "print('Test AUC score: ', test_auc)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter \"k\" value to check precision at \"k\"\n",
    "\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_precision = precision_at_k(model_hybrid, \n",
    "                                 weights_train,\n",
    "                                 item_features=item_features, \n",
    "                                 num_threads=4, \n",
    "                                 k=k).mean()\n",
    "\n",
    "print('Train precision at k: ', train_precision)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "test_precision = precision_at_k(model_hybrid, \n",
    "                                weights_test, \n",
    "                                item_features=item_features, \n",
    "                                train_interactions = weights_train, \n",
    "                                num_threads=4, \n",
    "                                k=k).mean()\n",
    "\n",
    "print('Test precision at k: ', test_precision)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train set into train_train and validate subsets.\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "\n",
    "(train_train, validate) = random_train_test_split(weights_train, \n",
    "                                                  test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Yield possible hyperparameter choices.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.randint(10, 200),\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"loss\": np.random.choice([\"bpr\", \"warp\"]),\n",
    "            \"learning_rate\": np.random.exponential(0.05),\n",
    "            \"item_alpha\": np.random.exponential(1e-10),\n",
    "            \"max_sampled\": np.random.randint(5, 15),\n",
    "            \"num_epochs\": np.random.randint(10, 100),\n",
    "            \"random_state\": 2020,\n",
    "        }\n",
    "\n",
    "\n",
    "def random_search(train, validate, num_samples=20, k=10, num_threads=4):\n",
    "    \"\"\"\n",
    "    Sample random hyperparameters, fit a LightFM model, and evaluate it\n",
    "    on the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    train: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Training data.\n",
    "    validate: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Validation data.\n",
    "    num_samples: int, optional\n",
    "        Number of hyperparameter choices to evaluate.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    generator of (precision_at_k, hyperparameter dict, fitted model)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for hyperparams in itertools.islice(sample_hyperparameters(), num_samples):\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "\n",
    "        model_tuned = LightFM(**hyperparams)\n",
    "        model_tuned.fit(train, epochs=num_epochs, num_threads=num_threads)\n",
    "\n",
    "        ranking_score = precision_at_k(model_tuned, validate, train_interactions=train, \n",
    "                                       num_threads=num_threads, \n",
    "                                       k=k).mean()\n",
    "\n",
    "        hyperparams[\"num_epochs\"] = num_epochs\n",
    "\n",
    "        yield (ranking_score, hyperparams, model_tuned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    (ranking_score, hyperparams, model_tuned) = max(random_search(train_train, validate), \n",
    "                                                    key=lambda x: x[0])\n",
    "\n",
    "    print('Best presicion ranking score {} at {}'.format(ranking_score, hyperparams))\n",
    "    print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LightFM model training (with item features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tuned model.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_hybrid = model_tuned\n",
    "\n",
    "model_hybrid.fit(weights_train, \n",
    "                 item_features=item_features, \n",
    "                 epochs=hyperparams['num_epochs'], \n",
    "                 num_threads=4)\n",
    "\n",
    "print('Model trained in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned model evaluation (auc_score, precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_auc = auc_score(model_hybrid, \n",
    "                      weights_train, \n",
    "                      item_features=item_features, \n",
    "                      num_threads=4).mean()\n",
    "\n",
    "print('Train AUC score: ', train_auc)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train interactions fill be also passed to avoid model re-recommending items to users.\n",
    "test_auc = auc_score(model_hybrid,\n",
    "                     weights_test, \n",
    "                     item_features=item_features, \n",
    "                     train_interactions = weights_train, \n",
    "                     num_threads=4).mean()\n",
    "\n",
    "print('Test AUC score: ', test_auc)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter \"k\" value to check precision at \"k\"\n",
    "\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_precision = precision_at_k(model_hybrid, \n",
    "                                 weights_train,\n",
    "                                 item_features=item_features, \n",
    "                                 num_threads=4, \n",
    "                                 k=k).mean()\n",
    "\n",
    "print('Train precision at k: ', train_precision)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "test_precision = precision_at_k(model_hybrid, \n",
    "                                weights_test, \n",
    "                                item_features=item_features, \n",
    "                                train_interactions = weights_train, \n",
    "                                num_threads=4, \n",
    "                                k=k).mean()\n",
    "\n",
    "print('Test precision at k: ', test_precision)\n",
    "print('Calculated in: ', round((time.time()-start_time)/60, 2), \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
